{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qor_1fLyoiri"
      },
      "source": [
        "### Exercises\n",
        "\n",
        "1\\. Describe in writing why cross entropy performs better as a loss function then mean squared error.\n",
        "\n",
        "2\\. Describe in writing why dropout allows to reduce overfitting.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Cross-entropy performs better as a loss function then mean squared error because cross-entropy is less sensitive to heavy skew (\"тяжелые\" хвосты). Cross-entropy, penalizes misifications logarithmically, which reduces the impact of extreme prediction errors. Cross-entropy is also provides steeper gradients. This is beneficial during gradient descent optimization, as steeper gradients can lead to faster convergence."
      ],
      "metadata": {
        "id": "XUMj61RupPMW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Dropout prevents complex co-adaptations of neurons during training by randomly deactivating a fraction of neurons in each iteration. This process forces the network to learn more robust features, as different combinations of neurons are activated during each training iteration. By randomly dropping out neurons, the network becomes less reliant on specific neurons during training."
      ],
      "metadata": {
        "id": "HvHXREEvqZ1d"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RT2fQ43toirs"
      },
      "source": [
        "### Exercises\n",
        "\n",
        "\n",
        "5\\. Describe in writing why pooling is necessary after convolution.\n",
        "\n",
        "6\\. Describe in writing why dropout layer are not recommended after convolutional layers.\n",
        "\n",
        "8\\. Besides MNIST `tensorflow` provides many more commonly used datasets. With the command\n",
        "\n",
        "`(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()`\n",
        "\n",
        "you can open a dataset called CIFAR-10.\n",
        "\n",
        "The CIFAR10 dataset contains 60000 color images in 10 classes, with 6000 images in each class. The dataset is divided into 50000 training images and 10000 testing images. The classes are mutually exclusive and there is no overlap between them.\n",
        "\n",
        "Class names are as follows:\n",
        "\n",
        "`class_names = ['airplane', 'automobile', 'bird', 'cat',\n",
        "                'deer', 'dog', 'frog', 'horse', 'ship', 'truck']`\n",
        "\n",
        "Create a convolutional network that is able to predict these images."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. By applying pooling, we can reduce the spatial dimensions. This helps in controlling the number of parameters and computations in the network and assists in preventing overfitting. This makes the network more robust by focusing on general patterns rather than precise details. Pooling operations help in extracting dominant features from the convolved features."
      ],
      "metadata": {
        "id": "OM5-ooU1wwwi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Adding dropout layers after convolutional layers increases the computational complexity of the network, as dropout introduces additional operations during  training. This can slow down training and inference times without significant benefits. Also it randomly sets some activations to zero during training, which can disrupt the learned spatial hierarchies and hence degrade the performance of the network.\n",
        "\n"
      ],
      "metadata": {
        "id": "eehbSFihxO3l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n",
        "\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print('Test accuracy:', round(test_acc, 4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qxjz0oI2xGpS",
        "outputId": "10afbb85-bed6-4725-8c31-5bafd55b579a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1563/1563 [==============================] - 80s 50ms/step - loss: 1.5362 - accuracy: 0.4366 - val_loss: 1.2440 - val_accuracy: 0.5530\n",
            "Epoch 2/10\n",
            "1563/1563 [==============================] - 75s 48ms/step - loss: 1.1777 - accuracy: 0.5832 - val_loss: 1.0957 - val_accuracy: 0.6147\n",
            "Epoch 3/10\n",
            "1563/1563 [==============================] - 73s 47ms/step - loss: 1.0216 - accuracy: 0.6381 - val_loss: 0.9853 - val_accuracy: 0.6515\n",
            "Epoch 4/10\n",
            "1563/1563 [==============================] - 72s 46ms/step - loss: 0.9152 - accuracy: 0.6757 - val_loss: 0.9721 - val_accuracy: 0.6588\n",
            "Epoch 5/10\n",
            "1563/1563 [==============================] - 71s 45ms/step - loss: 0.8463 - accuracy: 0.7019 - val_loss: 0.9249 - val_accuracy: 0.6811\n",
            "Epoch 6/10\n",
            "1563/1563 [==============================] - 74s 47ms/step - loss: 0.7830 - accuracy: 0.7247 - val_loss: 0.8761 - val_accuracy: 0.6996\n",
            "Epoch 7/10\n",
            "1563/1563 [==============================] - 72s 46ms/step - loss: 0.7376 - accuracy: 0.7411 - val_loss: 0.8987 - val_accuracy: 0.6887\n",
            "Epoch 8/10\n",
            "1563/1563 [==============================] - 73s 47ms/step - loss: 0.6945 - accuracy: 0.7561 - val_loss: 0.8339 - val_accuracy: 0.7155\n",
            "Epoch 9/10\n",
            "1563/1563 [==============================] - 74s 47ms/step - loss: 0.6554 - accuracy: 0.7685 - val_loss: 0.8974 - val_accuracy: 0.6920\n",
            "Epoch 10/10\n",
            "1563/1563 [==============================] - 74s 47ms/step - loss: 0.6248 - accuracy: 0.7797 - val_loss: 0.9016 - val_accuracy: 0.6997\n",
            "313/313 [==============================] - 5s 16ms/step - loss: 0.9016 - accuracy: 0.6997\n",
            "Test accuracy: 0.6997\n"
          ]
        }
      ]
    }
  ]
}