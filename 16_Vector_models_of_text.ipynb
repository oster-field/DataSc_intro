{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGnmBuP73odf"
      },
      "source": [
        "### Exercises\n",
        "\n",
        "1\\. Describe in writing why stopword removal is required when a vectorized model of a text is prepared.\n",
        "\n",
        "2\\. Describe in writing what are stemming and lemmatization. For what purpose they are leveraged?\n",
        "\n",
        "3\\. Describe in writing what are n-grams and why their using may improve a text model."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Stopwords appear frequently in a text but carry almost no meaningful information. By removing them, we can reduce the number of tokens processed by the model, leading to faster training. Also if including them in the vectorized model we may some significant noise so the model can't identify important patterns and relationships in the text data."
      ],
      "metadata": {
        "id": "wl3Bp1m730ET"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "2. *   **Stemming** is converting words into their base form which means stripping prefixes or suffixes . The goal of stemming is to cut words down to a root form. The resultant stem form may not be real words, the intent is to define a base form that can be found for the related words. **Lemmatization** is reducing words to its base form or dictionary form whereby the word has meaning to it. It is a kind of dictionary lookups and linguistic analysis to recover words to their dictionary form.\n",
        "*   The problem of multiple forms of the same word is addressed through stemming and lemmatization. Both techniques reduce the words to their base forms, which helps to standardize the words and reduce the feature space. As a result, these two techniques improve the accuracy of various tasks, such as text classification, sentiment analysis, and information retrieval.\n"
      ],
      "metadata": {
        "id": "HSUN3K67YLqv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. N-grams are contiguous sequences of n objects (generally words) in a body of text. N-gram consists of N (integer number) of adjacent words in sentence. This may be used to research the context and relationships among phrases in a given text. By shooting sequences of words, N-grams permit the model to recognize the context in which words seem and derive meaning from the relationships between adjoining phrases. The use of N-grams can improve a textual content version by using permitting it to find contextual relationships and extract more informative features for textual analysys tasks."
      ],
      "metadata": {
        "id": "NbqGDFzttmbj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUaHg6wnvBcw"
      },
      "source": [
        "### Exercises\n",
        "\n",
        "4\\. Describe in writing the key differences between BoW and TF-IDF models of text.\n",
        "\n",
        "5\\. Describe in writing what is an idea of word embedding. What are its advantages in comparison with other vectorization techniques?\n",
        "\n",
        "6\\. Come up with two sentences with high cosine similarity and two whose similarity is exactly zero. Compute these similarities using the code that has been used above.\n",
        "\n",
        "7\\. Compute word mover's distances for the sentences from the previous exercise. Use Word2vec model trained on `text8` corpus or download the pretrained model `glove-wiki-gigaword-50`. Compare the distances with cosine similarity. What method produces more reasonable results?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. While BoW treats all words equally, focusing only on their frequency in the document, TF-IDF considers the importance of a word not only in a document but also in the entire corpus through IDF. Also BoW vectors are typically normalized by the document length to mitigate bias towards longer documents, and TF-IDF normalizes term frequency based on the frequency of the term in the corpus."
      ],
      "metadata": {
        "id": "AQaYqm1gvT4K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Word embedding is a method used in natural language processing to symbolize phrases as dense, low-dimensional vectors in a continuous vector space. The basic idea is to encode semantic and syntactic similarities between words by using them more closely collectively in a vector space if they share comparable contexts or meanings. Word embedding techniques offer advantages in capturing semantic meaning, contextual information, and word relationships, providing more efficient and effective representations for natural language processing tasks compared to BoW or TF-IDF described before and unlike this vactorization techniques, word embedding captures nuanced contextual information and it can represent multiple meanings of a word."
      ],
      "metadata": {
        "id": "HawZhJjjwQmT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity #  Here is almost the same function as in lecture\n",
        "\n",
        "sentence1 = 'The moon shines brightly in the night sky today'\n",
        "sentence2 = 'Night falls as the moon fills the dark sky today'\n",
        "sentence3 = 'The sky blue so as the moon'\n",
        "sentence4 = 'Bananas are a good source potassium'\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform([sentence1, sentence2, sentence3, sentence4])\n",
        "cosine_similarities = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
        "print(f'Cosine Similarity of sentence 1 and sentence 2: {round(cosine_similarities[0, 1], 3)}')\n",
        "print(f'Cosine Similarity of sentence 3 and sentence 4: {cosine_similarities[2, 3]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ir__I3cnvysv",
        "outputId": "f3d52db0-9d93-4bff-b29f-e1adcab08172"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity of sentence 1 and sentence 2: 0.527\n",
            "Cosine Similarity of sentence 3 and sentence 4: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXfYegEt7sGj",
        "outputId": "edfb06bc-3ddc-475f-86e9-4deaf95186ca"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "import gensim.downloader as api\n",
        "\n",
        "word2vec_model = api.load('text8')\n",
        "\n",
        "sentences = [\n",
        "    'The moon shines brightly in the night sky today',\n",
        "    'Night falls as the moon fills the dark sky today',\n",
        "    'The sky blue so as the moon',\n",
        "    'Bananas are a good source of potassium'\n",
        "]\n",
        "\n",
        "tokenized_sentences = [sentence.split() for sentence in sentences]\n",
        "\n",
        "for i in range(len(tokenized_sentences)):\n",
        "    for j in range(i+1, len(tokenized_sentences)):\n",
        "        distance_wmd = word2vec_model.wv.wmdistance(tokenized_sentences[i], tokenized_sentences[j])  #  I don't know what's wrong here..\n",
        "        cosine_similarity = word2vec_model.wv.n_similarity(tokenized_sentences[i], tokenized_sentences[j])\n",
        "        print(f'Distance between Sentence {i+1} and Sentence {j+1} (WMD): {distance_wmd}')\n",
        "        print(f'Cosine Similarity between Sentence {i+1} and Sentence {j+1}: {cosine_similarity}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "APSyIZLI6zFL",
        "outputId": "2b10724a-a41e-450e-8a55-9c03f989f6fa"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'Dataset' object has no attribute 'wv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-be79abe1ff4c>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mdistance_wmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword2vec_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwmdistance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_sentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenized_sentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mcosine_similarity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword2vec_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_sentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenized_sentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Dataset' object has no attribute 'wv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8\\. Below you will find a piece of text. Split it to sentences and create BoW model. Above we have used stemming for the analogous model. Use lemmatization instead. Do not forget that lematization may require whole sentences to identify parts of speech. It means that the stopword removal must be done after lemmatization."
      ],
      "metadata": {
        "id": "Lbz9izSj6Szy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnmc8JxpDdwI",
        "outputId": "e53bb436-a532-4ef7-9ff5-a165522f67bf"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from collections import Counter\n",
        "\n",
        "text = \"\"\"The skull and the upper bones lay beside it in the thick dust, and in one place, where rain-water had dropped through a leak in the roof,\n",
        "the thing itself had been worn away. Further in the gallery was the huge skeleton barrel of a Brontosaurus. My museum hypothesis was confirmed.\n",
        "Going towards the side I found what appeared to be sloping shelves, and clearing away the thick dust,\n",
        "I found the old familiar glass cases of our own time. But they must have been air-tight to judge from the fair preservation of some of their contents.\"\"\"\n",
        "sentences = sent_tokenize(text)\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = []\n",
        "for sentence in sentences:\n",
        "    words = word_tokenize(sentence)\n",
        "    lemmatized_words.extend([wordnet_lemmatizer.lemmatize(word) for word in words])\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [word for word in lemmatized_words if word.lower() not in stop_words]\n",
        "print(filtered_words)\n",
        "bow_model = Counter(filtered_words)\n",
        "print(bow_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvhhQEPV6ToH",
        "outputId": "ca5e33c3-7b7b-468c-eb04-fe1dc8b597b8"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['skull', 'upper', 'bone', 'lay', 'beside', 'thick', 'dust', ',', 'one', 'place', ',', 'rain-water', 'dropped', 'leak', 'roof', ',', 'thing', 'worn', 'away', '.', 'gallery', 'wa', 'huge', 'skeleton', 'barrel', 'Brontosaurus', '.', 'museum', 'hypothesis', 'wa', 'confirmed', '.', 'Going', 'towards', 'side', 'found', 'appeared', 'sloping', 'shelf', ',', 'clearing', 'away', 'thick', 'dust', ',', 'found', 'old', 'familiar', 'glass', 'case', 'time', '.', 'must', 'air-tight', 'judge', 'fair', 'preservation', 'content', '.']\n",
            "Counter({',': 5, '.': 5, 'thick': 2, 'dust': 2, 'away': 2, 'wa': 2, 'found': 2, 'skull': 1, 'upper': 1, 'bone': 1, 'lay': 1, 'beside': 1, 'one': 1, 'place': 1, 'rain-water': 1, 'dropped': 1, 'leak': 1, 'roof': 1, 'thing': 1, 'worn': 1, 'gallery': 1, 'huge': 1, 'skeleton': 1, 'barrel': 1, 'Brontosaurus': 1, 'museum': 1, 'hypothesis': 1, 'confirmed': 1, 'Going': 1, 'towards': 1, 'side': 1, 'appeared': 1, 'sloping': 1, 'shelf': 1, 'clearing': 1, 'old': 1, 'familiar': 1, 'glass': 1, 'case': 1, 'time': 1, 'must': 1, 'air-tight': 1, 'judge': 1, 'fair': 1, 'preservation': 1, 'content': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MA5kad8FvBcw"
      },
      "source": [
        "9\\. Below you will find a list of tweets. Create TF-IDF model for them. For tokenization use TweetTokenizer provided by NLTK. Using cosine similarity find two most similar teats."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# List of tweets\n",
        "tweets = [\"@Tatiana_K nope they didn't have it \",\"@twittera que me muera ? \",\"spring break in plain city... it's snowing ))) \",\n",
        "          \"I just re-pierced my ears \",\"@caregiving I couldn't bear to watch it.  And I thought the UA losssssss was embarrassing . . . . .\",\n",
        "          \"@octolinz16 It it counts, idk why I did either. you never talk to me anymore \",\"@smarrison i would've been the first, but i didn't have a gun.  not really though, zac snyder's just a doucheclown.\",\n",
        "          \"@iamjazzyfizzle I wish I got to watch it with you!! I miss you and @iamlilnicki  how was the premiere?!\",\n",
        "          \"Hollis' death scene will hurt me severely to watch on film  wry is directors cut not out now?\",\"about to file taxes \",\n",
        "          \"@LettyA ahh ive always wanted to see rent  love the soundtrack!!\",\"@FakerPattyPattz Oh dear. Were you drinking out of the forgotten table drinks? \",\n",
        "          \"@alydesigns i was out most of the day so didn't get much done ;) \",\"one of my friend called me, and asked to meet with her at Mid Valley today...but i've no time *sigh* \"]\n",
        "\n",
        "tokenizer = TweetTokenizer()\n",
        "tokenized_tweets = [tokenizer.tokenize(tweet) for tweet in tweets]\n",
        "tokenized_tweets_str = [' '.join(tokens) for tokens in tokenized_tweets]\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(tokenized_tweets_str).toarray()\n",
        "cosine_similarities = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
        "np.fill_diagonal(cosine_similarities, 0)\n",
        "most_similar_indices = np.unravel_index(cosine_similarities.argmax(), cosine_similarities.shape)\n",
        "\n",
        "print('Most similar tweets:')\n",
        "print(tweets[most_similar_indices[0]])\n",
        "print(tweets[most_similar_indices[1]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJMrty-rDwmq",
        "outputId": "f807de13-8b9b-4acd-a8a0-31b5e8de65a4"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar tweets:\n",
            "@caregiving I couldn't bear to watch it.  And I thought the UA losssssss was embarrassing . . . . .\n",
            "@iamjazzyfizzle I wish I got to watch it with you!! I miss you and @iamlilnicki  how was the premiere?!\n"
          ]
        }
      ]
    }
  ]
}