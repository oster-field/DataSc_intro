{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfUiycT5SqhK"
      },
      "source": [
        "\n",
        "\n",
        "1. What is vector norm and what vector norms you know?\n",
        "**A vector norm is a mathematical functional that assigns a non-negative scalar value to a vector in a vector space. The most common is Euclidean norm (the square root of the sum of the squares of its components). Also from functional analysis I remember L1-norm (the sum of the absolute values of vector components) and Lâˆž-norm (the maximum absolute value of components)**\n",
        "\n",
        "2. What is cosine similarity?\n",
        "**Cosine similarity is a measure of similarity between two non-zero vectors that measures the cosine of the angle between them. The cosine similarity levels from -1 to one, with 1 indicating that the vectors are same, 0 indicating no similarity, and -1 indicating entire dissimilarity. It is often used natural language processing to examine the similarity of words.**\n",
        "3. What are mean, median and mode of a dataset?\n",
        "**The mean is the average of all the values in a dataset. It is calculated by adding up all the values in the dataset and then dividing by the total number of values. The median is the middle value in a dataset when the values are arranged (sorted) in numerical order. If there is an even number of values, the median is the average of the two middle values. The mode is the value that appears most frequently (the frequency of each element is calculated as the ratio of the number of occurrences of that element to the total number of elements) in a dataset. If no value appears more than once, the dataset is said to have no mode.**\n",
        "4. What are quartiles?\n",
        "**Quartiles are values that divide a dataset into four equal parts. The first quartile (Q1) represents the value below which 25% of the data falls, the second quartile (Q2) represents the median or middle value of the data set, and the third quartile (Q3) represents the value below which 75% of the data falls.**\n",
        "5. What are two ways to define probability?\n",
        "**Classical definition: this approach probability as the ratio of favorable outcomes to total number of equally outcomes in a given sample space. Relative Frequency: relative frequency probability is based on observed frequencies or historical data. It calculates the probability of an event based on the frequency with which it occurs relative to the total number of trials or observations.**\n",
        "6. What are independent random events?\n",
        "**Independent random events are events where the occurrence of one event does not affect the probability of the other event occurring. In other words, the outcome of one event has no impact on the outcome of the other event.**\n",
        "7. What is normal distribution?\n",
        "**Normal (Gaussian) distribution is a type of probability distribution that is symmetrical around its mean. It's PDF characterized by a bell-shaped curve, with the majority of values falling near the mean and fewer values further away from the mean. In a normal distribution, approximately 68% of data falls within one standard deviation of the mean, 95% falls within two standard deviations, and 99.7% falls within three standard deviations (the well-known 3-sigma theorem). CDF contains a special function: error function.**\n",
        "8. What is central limit theorem?\n",
        "**It's a theorem that states that the sum of a sufficiently large number of weakly dependent random variables that have approximately the same scales (none of the terms dominates or makes a determining contribution to the sum) has a distribution close to normal, and the larger this sum, the greater the distribution tends to be normal. In practice, the last part of this statement is easy to see, since it is true in a huge number of practical problems. For example, in my observations, on a linear scale the curve of the observed distribution often coincides with a Gaussian one already at the sum of one hundred elements, but to see this on a logarithmic scale you need about thousands of elements. I mean that in real life we do not observe infinite sums, and an exact resemblance to the Gaussian distribution cannot be obtained.**\n",
        "9. What is cross entropy?\n",
        "**Cross entropy is a mathematical formula used to quantify the difference between two probability distributions. In the context of machine learning, cross entropy is commonly used as a loss function in classification tasks to measure the difference between the predicted probability distribution output by a neural network and the true distribution of the labels in the training data. By minimizing the cross entropy loss, the neural network can learn to make more accurate predictions.**\n",
        "10. Describe Type 1 and Type 2 errors in statistical inference.\n",
        "**Type 1 error is the probability of rejecting a null hypothesis when it is actually true and hence this error occurs when a null hypothesis that is actually true is mistakenly rejected. Type 2 error is the probability of not rejecting a null hypothesis when it is actually false and hence type 2 error occurs when a null hypothesis that is actually false is mistakenly not rejected.**\n",
        "11. What is gradient decent?\n",
        "**Gradient descent is an optimization algorithm used to minimize a function by iteratively moving in the direction of steepest descent of the function. Algorithm calculates the gradient of the loss function with respect to the parameters and updates the parameters in the opposite direction of the gradient in order to minimize the loss function. That is, being in the N dimensional space of N parameters, as in classical mathematical analysis, we have the opportunity to find a strictly defined gradient (generally speaking, by the Laplace operator), which will show us the direction of growth of the function, and taking this gradient with a negative sign we will move towards reducing the error .**\n",
        "12. What is CSV-file?\n",
        "**A CSV (Comma-Separated Values) file is a plain text file that stores tabular data in a format where each line represents a row, and columns are separated by commas. Columns may have names, names could reflect the types (numbers or textual data for example) of data stored within them.**\n",
        "13. What data outliers?\n",
        "**Data outliers are data points that are significantly different from the rest of the data in a dataset. These outliers can skew statistical analyses and distort interpretations of the data. They can be easily determined by plotting the data distribution density discrete analog (histogram); outliers will be in the tails of the function. They can also be easily recognized by calculating the variance of the data, then the root of the variance will show a characteristic scale of values by which outliers can be easily identified. Most often they appear as a result of measurement error (for example, temperature -300 Kelvin).**\n",
        "14. What are regular expressions?\n",
        "**It is the sequences of characters that define search patterns. They are usually used for pattern matching in strings, allowing to search for and match specific strings based on a certain pattern or format**\n",
        "15. What matches the pattern `[\\w']+`\n",
        "**[\\w'] means a character class that can match any letter, digit, or underscore (\\w), or an apostrophe (').**\n",
        "16. What is one-hot encoding? When it is applied?\n",
        "**One-hot encoding is a technique used in machine learning and data analysis to convert categorical variables into a format (matrix of vectors) that can be provided to machine learning algorithms. Each category of a variable is represented as a binary vector with one element set to 1 and all other elements set to 0. Each category is then represented by a unique binary vector. One-hot encoding is typically applied to categorical variables (like car brands or proper names) before giving the data into a machine learning algorithm, as many algorithms require numerical input.**\n",
        "17. What PCA does?\n",
        "**Principal Component Analysis (PCA) is a statistical technique used to reduce the dimensionality of data while retaining as much information as possible. It achieves this by transforming the original variables into a new set of uncorrelated variables called principal components. If there is a multidimensional construction, but some quantities depend on each other, say, quasilinearly, then dimensional reduction will help to see the really important (nonlinear and uncorrelated) dependencies.**\n",
        "18. Describe three types of machine learning models.\n",
        "**{I answered almost all this questions below in more detail earlier}. Supervised Learning: the model is trained on a labeled dataset, where each input data point is associated with a corresponding output label. The goal of the model is to learn the mapping between the input data and the output labels, so that it can make accurate predictions on new, unseen data.\n",
        "Unsupervised Learning the model is trained on an unlabeled dataset, where there are no output labels provided. The goal of the model is to find patterns or relationships in the data.\n",
        "Reinforcement Learning: the model learns through trial and error by interacting with an environment. The model receives feedback in the form of rewards or penalties based on its actions, and its objective is to maximize the cumulative reward over time.**\n",
        "19. When overfitting occurs? How to reduces it?\n",
        "**Overfitting occurs when a machine learning model performs too well on the training set but does not generalize well to new, unseen data. This can happen when the model is too complex and captures even noise in the training data. The ways to avoid overfitting is: cross-validation, regularization, dropout.**\n",
        "20. What is underfitting?\n",
        "**When the model is too simple and is not able to generalize well to new, unseen data (as well as to test data) and can't capture the underlying patterns.**\n",
        "21. Describe metrics usually used for regression models.\n",
        "**Mean Squared Error (MSE): the average squared difference between the estimated values and the actual values in a regression model.\n",
        "Root Mean Squared Error (RMS): RMS is the square root of the MSE.\n",
        "Mean Absolute Error (MAE): MAE calculates the average absolute differences between the predicted and actual values.\n",
        "R-squared (R2): R-squared measures the proportion of the variance in the dependent variable that is explained by the independent variables in the model.\n",
        "Mean Squared Percentage Error (MSPE): MSPE is a percentage-based alternative to MSE that calculates the average of the squared percentage difference between predicted and actual values.\n",
        "Mean Absolute Percentage Error (MAPE): MAPE calculates the average percentage difference between predicted and actual values.**\n",
        "22. Describe metrics usually used for classification models.\n",
        "**Accuracy: The proportion of correctly classified instances out of all instances.\n",
        "Precision: The proportion of true positive predictions out of all positive predictions made.\n",
        "Recall: The proportion of true positive predictions out of all actual positive instances.\n",
        "F1 score: The harmonic mean of precision and recall, it provides a balance between the two metrics.\n",
        "Confusion matrix: A table that summarizes the performance of a classification model by listing the counts of true positive, false positive, true negative, and false negative predictions.**\n",
        "23. Describe an idea of k-nearest neighbors model.\n",
        "**The algorithm classifies an unknown data point based on the majority class of its k (integer number) nearest neighbors in the feature space. The value of k is a parameter that must be specified and determines the number of neighbors to consider when making predictions.\n",
        "To make a prediction for a new data point, the model calculates the distance between the new point and all other data points in the training set using a distance metric (such as Euclidean distance described before). It then selects the k nearest neighbors and assigns the new point to the class that is most common among its neighbors.**\n",
        "24. Describe an idea of naive Bayes classifier.\n",
        "**Bayes classifier based on Bayes theorem with a naive assumption of independence between the features (hence, Central Limit Theorem, hense normal distibution). This assumption simplifies the calculations and makes the algorithm faster and more efficient.\n",
        "The idea is to determine the probability of a given instance belonging to a particular class based on the features of the instance. It calculates the conditional probability of each feature given the class and uses these probabilities to make predictions.**\n",
        "29. What is dropout?\n",
        "**Dropout is a technique used during training to prevent overfitting. During dropout, a random set of neurons are temporarily ignored (in other words, excluded from training), meaning their weights are not updated and their output is not included in the forward or backward pass. This forces the network to learn more robast features by preventing it from relying too heavily on any one particular set of neurons.**\n",
        "31. What is stemming and lammatization of a text?\n",
        "**They can reduce words to their base or root form. Stemming involves cutting off prefixes or suffixes of words to return the word to its stem. Lemmatization, involves reducing a word to its lemma (base form) by considering the context and meaning of the word.**\n",
        "32. What are stopwords and why they are usually removed?\n",
        "**Stopwords are common (most frequently usind, like articles, pronouns, prepositions, conjunctions and so on) words in a language that are usually removed from text data during natural language processing. These words that do not carry much meaning or significance in the context of the text and removing them before analysis can help speed up the work of the algorithms without qualitatively changing the result..**\n",
        "33. What vector models of a text you know?\n",
        "**I worked with Bag of Words (BoW), Term Frequency-Inverse Document Frequency (TF-IDF) and Word2Vec vector models of a text in section 16.**\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H-ZOWlUUw3QI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}